{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTz6FCf5kF6P"
      },
      "outputs": [],
      "source": [
        "#Install and import dependencies\n",
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from typing import List\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBj9L_9vlJoR"
      },
      "outputs": [],
      "source": [
        "#Locating the GPU and ensuring it doesn't use up all system memory\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "try:\n",
        "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LoVeiDclqP6"
      },
      "outputs": [],
      "source": [
        "import gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kte5H5mTmYZC"
      },
      "outputs": [],
      "source": [
        "#Download the data\n",
        "url = 'https://drive.google.com/uc?id=1YlvpDLix3S-U8fd-gqRwPcWXAXm8JwjL'\n",
        "output = 'data.zip'\n",
        "gdown.download(url, output, quiet = False)\n",
        "gdown.extractall('data.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE6_UAYWn9EB"
      },
      "outputs": [],
      "source": [
        "#Convert each video to frames\n",
        "def load_video(path:str) -> List[float]:\n",
        "  cap = cv2.VideoCapture(path)\n",
        "  frames = []\n",
        "  for x in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
        "    ret, frame = cap.read()\n",
        "    frame = tf.image.rgb_to_grayscale(frame)\n",
        "    frames.append(frame[190:236, 80:220, :])\n",
        "  cap.release()\n",
        "\n",
        "  #Normalise the data\n",
        "  mean = tf.math.reduce_mean(frames)\n",
        "  std = tf.math.reduce_std(tf.cast(frames, tf.float32))\n",
        "  return tf.cast((frames - mean), tf.float32)/std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPW5DLjzotnA"
      },
      "outputs": [],
      "source": [
        "#Define the set of characters we are expecting\n",
        "vocab = [x for x in \"abcdefghijklmnopqrstuvwxyz'?!123456789 \"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVrz6lzIsjGN"
      },
      "outputs": [],
      "source": [
        "#Convet sequence of chars to num codes and vice versa (tokenisation)\n",
        "\n",
        "char_to_num = tf.keras.layers.StringLookup(vocabulary = vocab, oov_token = \"\")\n",
        "num_to_char = tf.keras.layers.StringLookup(vocabulary = char_to_num.get_vocabulary(), oov_token = \"\", invert = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JIXfLDvs57g"
      },
      "outputs": [],
      "source": [
        "#Load the video labels\n",
        "def load_alignments(path:str) -> List[str]:\n",
        "  with open(path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "  tokens = []\n",
        "  for line in lines:\n",
        "    line = line.split()\n",
        "    if line[2] != 'sil':\n",
        "      tokens = [*tokens, ' ', line[2]]\n",
        "  return char_to_num(tf.reshape(tf.strings.unicode_split(tokens, input_encoding = 'UTF-8'), (-1)))[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju8iQWFGuJS0"
      },
      "outputs": [],
      "source": [
        "#Load all the data\n",
        "def load_data(path: str):\n",
        "  path = bytes.decode(path.numpy())\n",
        "  file_name = path.split('/')[-1].split('.')[0]\n",
        "  video_path = os.path.join('data', 's1', f'{file_name}.mpg')\n",
        "  alignment_path = os.path.join('data', 'alignments', 's1', f'{file_name}.align')\n",
        "  frames = load_video(video_path)\n",
        "  alignments = load_alignments(alignment_path)\n",
        "\n",
        "  return frames, alignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKw-gIspwJS4"
      },
      "outputs": [],
      "source": [
        "#Testing the load data function\n",
        "test_path = \"./data/s1/bbaf2n.mpg\"\n",
        "frames, alignments = load_data(tf.convert_to_tensor(test_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTkMZde_7Rjp",
        "outputId": "8ef39ea4-7c6d-4be6-88ac-2ddde832c59d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['b', 'i', 'n', ' ', 'b', 'l', 'u', 'e', ' ', 'a', 't', ' ', 'f', ' ', 't', 'w', 'o', ' ', 'n', 'o', 'w']\n"
          ]
        }
      ],
      "source": [
        "print([bytes.decode(x) for x in num_to_char(alignments.numpy()).numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZSVbfeV8XTM"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "def mappable_function(path:str) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  result = tf.py_function(load_data, [path], (tf.float32, tf.int64))\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOFXYKe8_aNE"
      },
      "outputs": [],
      "source": [
        "#Creating our data pipeline\n",
        "data = tf.data.Dataset.list_files('./data/s1/*.mpg')\n",
        "data = data.shuffle(500)\n",
        "data = data.map(mappable_function)\n",
        "data = data.padded_batch(2, padded_shapes = ([75, None, None, None], [40]))\n",
        "data = data.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_split, test_split, val_split = 0.6, 0.2, 0.2\n",
        "num_samples = len(data)\n",
        "train_num, test_num, val_num = int(num_samples * train_split), int(num_samples * test_split), int(num_samples * val_split)\n",
        "\n",
        "train = data.take(train_num)\n",
        "temp_data = data.skip(train_num)\n",
        "val = temp_data.take(val_num)\n",
        "test = temp_data.skip(val_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_09_SrQArOP"
      },
      "outputs": [],
      "source": [
        "#Designing the deep neural network\n",
        "#Importing classes\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, Activation, Reshape, SpatialDropout3D, BatchNormalization, TimeDistributed, Flatten\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn7RQ6ZXBZ6p"
      },
      "outputs": [],
      "source": [
        "#Defining the model architecture\n",
        "model = Sequential()\n",
        "model.add(Conv3D(128, 3, input_shape = (75, 46, 140, 1), padding = 'same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPool3D((1,2,2)))\n",
        "\n",
        "model.add(Conv3D(256, 3, padding = 'same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPool3D((1,2,2)))\n",
        "\n",
        "model.add(Conv3D(75, 3, padding = 'same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPool3D((1,2,2)))\n",
        "\n",
        "model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "model.add(Bidirectional(LSTM(128, kernel_initializer = 'Orthogonal', return_sequences = True)))\n",
        "model.add(Dropout(.5))\n",
        "\n",
        "model.add(Bidirectional(LSTM(128, kernel_initializer = 'Orthogonal', return_sequences = True)))\n",
        "model.add(Dropout(.5))\n",
        "\n",
        "model.add(Dense(char_to_num.vocabulary_size()+1, kernel_initializer = 'he_normal', activation = 'softmax'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UFyKaziGraT"
      },
      "outputs": [],
      "source": [
        "#Defining a learning rate scheduler\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch < 30:\n",
        "    return lr\n",
        "  else:\n",
        "    return lr * tf.math.exp(-0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fdzhk56CG0u3"
      },
      "outputs": [],
      "source": [
        "#Defining the CTC loss function\n",
        "def CTCloss(y_true, y_pred):\n",
        "  batch_length = tf.cast(tf.shape(y_true)[0], dtype = \"int64\")\n",
        "  input_length = tf.cast(tf.shape(y_pred)[1], dtype = \"int64\")\n",
        "  label_length = tf.cast(tf.shape(y_true)[1], dtype = \"int64\")\n",
        "\n",
        "  input_length = input_length * tf.ones(shape=(batch_length, 1), dtype = \"int64\")\n",
        "  label_length = label_length * tf.ones(shape=(batch_length, 1), dtype = \"int64\")\n",
        "\n",
        "  loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWsLHBT4IJrW"
      },
      "outputs": [],
      "source": [
        "#Produce a video example with the true words and predicted words\n",
        "\n",
        "class ProduceExample(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, dataset) -> None:\n",
        "    self.dataset = dataset.as_numpy_iterator()\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs = None) -> None:\n",
        "    data = self.dataset.next()\n",
        "    yhat = self.model.predict(data[0])\n",
        "    decoded = tf.keras.backend.ctc_decode(yhat, [75, 75], greedy = False)[0][0].numpy()\n",
        "    for x in range(len(yhat)):\n",
        "      print('Original:', tf.strings.reduce_join(num_to_char(data[1][x])).numpy().decode('utf-8'))\n",
        "      print('Prediction:', tf.strings.reduce_join(num_to_char(decoded[x])).numpy().decode('utf-8'))\n",
        "      print('~'*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGu3QfMKKsGe"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = Adam(learning_rate = 0.001), loss=CTCloss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKw6YMcWKw2p"
      },
      "outputs": [],
      "source": [
        "checkpoint = ModelCheckpoint(os.path.join('model', 'checkpoint'), monitor = 'loss', save_weights_only = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nmKsUX_Laiv"
      },
      "outputs": [],
      "source": [
        "schedule_callback = LearningRateScheduler(scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz2NW7ZbLj4T"
      },
      "outputs": [],
      "source": [
        "example_callback = ProduceExample(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fo-NAmleL1gl",
        "outputId": "da560968-147e-4039-c10c-520855e8270f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            " 64/300 [=====>........................] - ETA: 3:38:18 - loss: 105.7146"
          ]
        }
      ],
      "source": [
        "history = model.fit(train, validation_data = val, epochs = 100, callbacks = [checkpoint, schedule_callback, example_callback], verbose = 1, batch_size = 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjNNhKt2OhyJ"
      },
      "outputs": [],
      "source": [
        "test_data = test.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlx6-G-RPZ6M"
      },
      "outputs": [],
      "source": [
        "sample = test_data.next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmVJPp57RUhd",
        "outputId": "b956ac0d-4618-4c6d-f690-cfa8fbe449f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 7s 7s/step\n"
          ]
        }
      ],
      "source": [
        "yhat = model.predict(sample[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF9cWkxARZXD",
        "outputId": "47281bb6-9c70-4608-c561-7482a0389b9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Actual text\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=string, numpy=b'bin white in g zero now'>,\n",
              " <tf.Tensor: shape=(), dtype=string, numpy=b'lay white by l nine again'>]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print('~'*50, 'Actual text')\n",
        "[tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in sample[1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yST4sg6RRq67"
      },
      "outputs": [],
      "source": [
        "decoded = tf.keras.backend.ctc_decode(yhat, input_length = [75, 75], greedy = True)[0][0].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg5kfCHlR1TD",
        "outputId": "64b1f80b-1eac-451f-c1ed-d8ad06b3a611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Predictions\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=string, numpy=b'bin white in g zero now'>,\n",
              " <tf.Tensor: shape=(), dtype=string, numpy=b'lay white by l nine again'>]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print('~'*50, 'Predictions')\n",
        "[tf.strings.reduce_join([num_to_char(word) for word in sentence]) for sentence in decoded]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVN3RPOyWoYz",
        "outputId": "848e2d74-46ca-4bb6-8905-7b11c76bf4da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1001"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(os.listdir('/content/data/s1'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}